{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-based recommendations\n",
    "Emanuel de Jong (495804) - Erik Markvoort (519894)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "these global variables are the index of the movie which the reccomendations will be based on and the number of reccomendations that should be given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_MOVIE_INDEX = 0\n",
    "N = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "these modules will be used to complete the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Std\n",
    "import re\n",
    "\n",
    "# Local\n",
    "from movie_display import movie_display\n",
    "\n",
    "# Third party\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.display import HTML\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from ipywidgets import interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the imdbdata JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = pd.read_json(\"dataset/imdbdata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global vars\n",
    "a library to store vectors of similarity cosines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "the function clean_persons is a repeated process where a list of names is transformed into a string which can be applied for a bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_persons(persons):\n",
    "    if isinstance(persons, str):  # Ensure the entry is a string\n",
    "        persons = re.sub(r'\\(.*?\\)', '', persons).strip()\n",
    "        persons = persons.replace(\"N/A\", \"\")\n",
    "        persons = persons.replace(\" \", \"\")\n",
    "        persons = persons.replace(\",\", \" \")\n",
    "        persons = persons.strip()\n",
    "    return persons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actors BoW\n",
    "For features that mostly consist of names a bag of words is applied, this includes actors, writers and directors. stemming is not applied and stopwords are not removed as names should remain unchanged. a bag of words is applied for names, and not a tf-idf since it is expected that a name does not appear more than once per movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a bag of words of actors\n",
    "def set_actors_bow():\n",
    "    actorRows = imdb['Actors'].apply(clean_persons)\n",
    "\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    bow_actors = count_vectorizer.fit_transform(actorRows)\n",
    "\n",
    "    feature_vectors['Actors'] = bow_actors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writers BoWs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_writers_bow():\n",
    "    writerRows= imdb['Writer'].apply(clean_persons)\n",
    "    writerRows[0]\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    bow_writer = count_vectorizer.fit_transform(writerRows)\n",
    "\n",
    "    feature_vectors['Writer'] = bow_writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Director BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_director_bow():\n",
    "    imdb['Director'] = imdb['Director'].apply(clean_persons)\n",
    "    directorRows = imdb[\"Director\"]\n",
    "\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    bow_director = count_vectorizer.fit_transform(directorRows)\n",
    "\n",
    "    feature_vectors['Director'] = bow_director"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title BoW\n",
    "similar to names, titles are not expected to have repeating words, which motivats a choice for a bag of words. a difference, however is the use of a stopwords filter, as titles are expected to have stopwords such as 'the' often. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_title_bow():\n",
    "    sw = set(stopwords.words(\"english\"))\n",
    "    title_rows = imdb[\"Title\"]\n",
    "\n",
    "    title_sentences = []\n",
    "    for title in title_rows:\n",
    "        title += \" \"\n",
    "        words = re.findall(r\"\\b\\w+(?:\\.\\w+)*(?:'\\w+)?\", title)\n",
    "        title_sentences.append(words)\n",
    "    filtered_titles = []\n",
    "    for words in title_sentences:\n",
    "        filtered = [w for w in words if not w.lower() in sw]\n",
    "        title = \"\"\n",
    "        for word in filtered:\n",
    "            title += word + \" \"\n",
    "        filtered_titles.append(title)\n",
    "        count_vectorizer = CountVectorizer()\n",
    "    bow_title = count_vectorizer.fit_transform(filtered_titles)\n",
    "\n",
    "    feature_vectors['Title'] = bow_title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot TF-IDF\n",
    "the data for the plot is handled differently, as it is expectd that words can appear more than once, which suggests the use of a tf-idf. stemming and stopwords are also applied since plots are full sentences, if not multiple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_plot_tf_idf():\n",
    "    sw = set(stopwords.words(\"english\"))\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    plot_rows = imdb[\"Plot\"]\n",
    "\n",
    "    plot_sentences = []\n",
    "    for plot in plot_rows:\n",
    "        plot += \" \"\n",
    "        words = re.findall(r\"\\b\\w+(?:\\.\\w+)*(?:'\\w+)?\", plot)\n",
    "        for i in range(len(words)):\n",
    "            words[i] = stemmer.stem(words[i])\n",
    "        plot_sentences.append(words)\n",
    "        filtered_plots = []\n",
    "    for words in plot_sentences:\n",
    "        filtered = [w for w in words if not w.lower() in sw]\n",
    "        filtered_plots.append(filtered)\n",
    "\n",
    "    tf_idf_vectorizer = TfidfVectorizer()\n",
    "    ids = imdb['imdbId'].tolist()\n",
    "    filtered_plot_strings = []\n",
    "\n",
    "    for imdb_id, filtered_plot in zip(ids, filtered_plots):\n",
    "        try:\n",
    "            if isinstance(filtered_plot, list):\n",
    "                filtered_plot = ' '.join(filtered_plot)\n",
    "            \n",
    "            filtered_plot_strings.append(filtered_plot)\n",
    "            \n",
    "        except ValueError as e:\n",
    "            print(f\"Error with IMDb ID {imdb_id}: {e}\")\n",
    "            pass\n",
    "\n",
    "    td_idf_plots = tf_idf_vectorizer.fit_transform(filtered_plot_strings)\n",
    "\n",
    "    feature_vectors['Plot'] = td_idf_plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the similar movies\n",
    "For each feature, actors, writers, directors, title and plot, a vector is made out of cosine similarities which stores the similarity of movies per movie. after this we will have 5 matrices of similarities. a weight is applied, making the similarity between plots somewhat more important than other similarities.\n",
    "the matrices of cosine similarities are merged by adding each vector together and dividing it by the number of features used to measure similarity, resulting in the mean of similarities.\n",
    "with all the similarities merged, a reccomendation can be made. a list of reccomendations is made, saving the index of the movie(id) and its similarity. after this, this list is sorted with the highest similarity first. the global variable is taken to make a top N movie list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_movies = []\n",
    "def get_similar_movies():\n",
    "    global similar_movies\n",
    "    cosine_similarities = []\n",
    "    for feature in feature_vectors.keys():\n",
    "        cosine_sim = cosine_similarity(feature_vectors[feature])\n",
    "        cosine_sim = MinMaxScaler().fit_transform(cosine_sim)\n",
    "\n",
    "        if (feature == 'Plot'):\n",
    "            cosine_sim *= 1.25\n",
    "        \n",
    "        cosine_similarities.append(cosine_sim)\n",
    "\n",
    "    combined_matrix = cosine_similarities[0]\n",
    "    for i in range(1, len(cosine_similarities)):\n",
    "        combined_matrix += cosine_similarities[i]\n",
    "\n",
    "    combined_matrix /= len(cosine_similarities)\n",
    "    combined_matrix[:5, :5]\n",
    "    recommendations = []\n",
    "    target_matrix = combined_matrix[TARGET_MOVIE_INDEX]\n",
    "    for i in range(0, len(target_matrix)):\n",
    "        if i == TARGET_MOVIE_INDEX:\n",
    "            continue\n",
    "\n",
    "        recommendations.append((i, target_matrix[i]))\n",
    "\n",
    "    recommendations = sorted(recommendations, key=lambda r: r[1], reverse=True)\n",
    "\n",
    "    similar_movies = [r[0] for r in recommendations[:N]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of showing movie information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('./dataset/imdbdata.json', orient='columns')\n",
    "movies_for_display = []\n",
    "def display_movies():\n",
    "    for i in range(len(similar_movies)):\n",
    "        movies_for_display.append(df.iloc[similar_movies[i]])\n",
    "\n",
    "def html_display():\n",
    "    display(HTML(movie_display.show(movies_for_display)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_widget = widgets.Text(\n",
    "        description=f'movie:',\n",
    "    )\n",
    "\n",
    "recommendation_slider = widgets.IntSlider(\n",
    "    min=1, max=20, step=1, value=10, description=\"Amount of recommendations:\"\n",
    ")\n",
    "\n",
    "# Button to submit the selections\n",
    "submit_button = widgets.Button(description=\"Submit\")\n",
    "\n",
    "# Function to trigger when button is clicked\n",
    "def on_submit_clicked(b):\n",
    "    global N\n",
    "    global TARGET_MOVIE_INDEX\n",
    "    N = recommendation_slider.value\n",
    "    TARGET_MOVIE_INDEX = int(text_widget.value)\n",
    "    set_actors_bow()\n",
    "    set_director_bow()\n",
    "    set_title_bow()\n",
    "    set_writers_bow()\n",
    "    set_plot_tf_idf()\n",
    "    get_similar_movies()\n",
    "    display_movies()\n",
    "    html_display()\n",
    "\n",
    "# Attach the button to the callback function\n",
    "submit_button.on_click(on_submit_clicked)\n",
    "\n",
    "# Display the widgets\n",
    "display(text_widget)\n",
    "display(recommendation_slider)\n",
    "display(submit_button)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
